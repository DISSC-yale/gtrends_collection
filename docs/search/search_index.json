{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Google Trends Collection Framework","text":"<p>This is in part a simple Python package to handle collection from the Google Trends beta research API, and a minimal framework to organize historical and continuous collection.</p>"},{"location":"#data","title":"Data","text":"<p>A selection of data are collected in the <code>data</code> directory, and are updated weekly.</p> <p>The selection is defined by the files in the <code>scope</code> directory.</p>"},{"location":"#local-use","title":"Local Use","text":"<p>To work with the data locally, you can clone this repository:</p> <pre><code>git clone --depth=1 https://github.com/DISSC-yale/gtrends_collection.git\n</code></pre> <p>Then load the data in Python:</p> <pre><code>from pyarrow.dataset import dataset\n\ndata = dataset(\"gtrends_collection/data\").to_table().to_pandas()\n</code></pre> <p>or R:</p> <pre><code>library(arrow)\n\ndata &lt;- open_dataset(\"gtrends_collection/data\") |&gt; dplyr::collect()\n</code></pre>"},{"location":"#collection","title":"Collection","text":"<p>The <code>scripts/historical_collection.py</code> script is used to collect full histories based on the scope files:</p> <pre><code>python scripts/historical_collection.py\n</code></pre> <p>The <code>scripts/weekly_collection.py</code> script is used by the GitHub Actions workflow to add new data each week.</p>"},{"location":"#authentication","title":"Authentication","text":"<p>A developer key is required to collect from the beta API.</p> <p>This can either be set to the <code>GOOGLE_API_KEY</code> environment variable, or stored in an <code>.env</code> file:</p> <pre><code>GOOGLE_API_KEY=AlphanumericKey\n</code></pre>"},{"location":"Data/","title":"Data","text":"<p>Summaries of the data collected as of 12:07:27 AM UTC on 2025-03-16</p>"},{"location":"Data/#locations","title":"Locations","text":"<ul> <li>Observations</li> <li>Means</li> </ul>"},{"location":"Data/#dates","title":"Dates","text":"term min max Respiratory syncytial virus (/g/11hy9m64ws) 2003-12-28 2025-03-02 Respiratory syncytial virus vaccine (/g/11j30ybfx6) 2003-12-28 2025-03-09 Bronchiolitis obliterans (/m/0b7k33) 2003-12-28 2025-03-02 Influenza (/m/0cycc) 2003-12-28 2025-03-16 bronchiolitis 2003-12-28 2025-03-02 influenza 2003-12-28 2025-03-16 nirsevimab 2003-12-28 2025-03-02 rsv 2003-12-28 2025-03-02"},{"location":"Data/#values","title":"Values","text":"term min mean std max Respiratory syncytial virus (/g/11hy9m64ws) 0.00 80.98 5889.15 4434828.10 Respiratory syncytial virus vaccine (/g/11j30ybfx6) 0.00 66.17 9456.00 5368845.95 Bronchiolitis obliterans (/m/0b7k33) 0.00 52.12 4624.37 2077404.38 Influenza (/m/0cycc) 0.00 2477.52 6522.85 1763593.87 bronchiolitis 0.00 50.46 5464.18 2642999.99 influenza 0.00 187.57 3661.17 1362586.92 nirsevimab 0.00 33.83 2763.01 1325261.88 rsv 0.00 278.41 3065.48 1175033.41"},{"location":"Framework/","title":"Framework","text":""},{"location":"Framework/#components","title":"Components","text":"<p>The Framework aspect of this repository consists of the following components:</p> <ol> <li>The <code>GOOGLE_API_KEY</code> environment variable, which may be stored in a <code>.env</code> file.</li> <li>A pair of <code>scope</code> files to define the main collection targets.</li> <li><code>scripts</code> to define different collection tasks.</li> <li>A <code>data</code> output directory to write results to.</li> </ol> <p>These make up a structure around which data are collected. By default, they are assumed to be in the root directory, but each can be redirected with function arguments.</p>"},{"location":"Framework/#automatic-updates","title":"Automatic Updates","text":"<p>On top of those components, the <code>.github/workflows/weekply_collection.yaml</code> file defines a GitHub action used to perform regular updates.</p> <p>This depends on the <code>GOOGLE_API_KEY</code> being defined in actions secretes (Settings &gt; Secrets and variables &gt; ACtions &gt; Secrets) and actions having write permissions (Settings &gt; Actions &gt; General &gt; Workflow permissions).</p>"},{"location":"functions/Collector/","title":"Collector","text":"<p>Collect internet search volumes from the Google Trends timeline for health endpoint.</p> <p>See the schema for more about the API. Only the <code>getTimelinesForHealth</code> endpoint is used here.</p> <p>Parameters:</p> Name Type Description Default <code>scope_dir</code> <code>str</code> <p>Directory containing the <code>terms.txt</code> and <code>locations.txt</code> files. See Specification.</p> <code>'scope'</code> <code>key_dir</code> <code>str</code> <p>Directory containing a <code>.env</code> file, to extract the <code>GOOGLE_API_KEY</code> variable from, if it is not already in the environment.</p> <code>'.'</code> <code>terms_per_batch</code> <code>int</code> <p>Maximum terms to include in each collection batch. Theoretically 30 is the API's max, but more than 1 seems to not work.</p> <code>1</code> <code>wait_time</code> <code>float</code> <p>Seconds to wait between each batch.</p> <code>0.1</code> <code>version</code> <code>str</code> <p>Version of the service API.</p> <code>'v1beta'</code> Specification <p>To process in batches, search terms and locations must be specified in separate files (<code>terms.txt</code> and <code>locations.txt</code>), stored in the <code>scope_dir</code> directory. These should contain 1 term / location code per line.</p> Collection Process <p>Initializing this class retrieves the Google API service, stores the developer key, and points to the scope directory.</p> <p>The <code>process_batches()</code> method reads in the terms and locations, and collects them in batches over the specified time frame.</p> <p>Results from each batch are stored in the <code>batches</code> property, which can be pulled from in case the <code>process_batches</code> process does not complete (such as if the daily rate limit is reached).</p> <p>The <code>collect()</code> method collects a single batch, and can be used on its own.</p> <p>Examples:</p> <pre><code>from gtrends_collection import Collector\n\n# initialize the collector\ncollector = Collector()\n</code></pre> Source code in <code>gtrends_collection/collector.py</code> <pre><code>class Collector:\n    \"\"\"\n    Collect internet search volumes from the Google Trends timeline for health endpoint.\n\n    See the [schema](https://trends.googleapis.com/$discovery/rest?version=v1beta)\n    for more about the API. Only the `getTimelinesForHealth` endpoint is used here.\n\n    Args:\n        scope_dir (str): Directory containing the `terms.txt` and `locations.txt` files.\n            See Specification.\n        key_dir (str): Directory containing a `.env` file, to extract the\n            `GOOGLE_API_KEY` variable from, if it is not already in the environment.\n        terms_per_batch (int): Maximum terms to include in each collection batch.\n            Theoretically 30 is the API's max, but more than 1 seems to not work.\n        wait_time (float): Seconds to wait between each batch.\n        version (str): Version of the service API.\n\n    Specification:\n        To process in batches, search terms and locations must be specified in separate\n        files (`terms.txt` and `locations.txt`), stored in the `scope_dir` directory.\n        These should contain 1 term / location code per line.\n\n    Collection Process:\n        Initializing this class retrieves the Google API service, stores the\n        developer key, and points to the scope directory.\n\n        The `process_batches()` method reads in the terms and locations,\n        and collects them in batches over the specified time frame.\n\n        Results from each batch are stored in the `batches` property,\n        which can be pulled from in case the `process_batches` process does not complete\n        (such as if the daily rate limit is reached).\n\n        The `collect()` method collects a single batch, and\n        can be used on its own.\n\n    Examples:\n        ```python\n        from gtrends_collection import Collector\n\n        # initialize the collector\n        collector = Collector()\n        ```\n    \"\"\"\n\n    # time to wait between requests\n    _regular_wait_time = 0.1\n    # time to wait after a `rateLimitExceeded` error\n    _fallback_wait_time = 2\n    batches: ClassVar[List[DataFrame]] = []\n\n    scope_dir = \"scope\"\n    max_terms = 1\n\n    def __init__(self, scope_dir=\"scope\", key_dir=\".\", terms_per_batch=1, wait_time=0.1, version=\"v1beta\"):\n        self._regular_wait_time = wait_time\n        self.scope_dir = scope_dir\n        self.max_terms = terms_per_batch\n\n        key = getenv(\"GOOGLE_API_KEY\")\n        if not key and isfile(f\"{key_dir}/.env\"):\n            with open(f\"{key_dir}/.env\", encoding=\"utf-8\") as content:\n                for pair in content.read().split(\"\\n\"):\n                    name, value = pair.split(\"=\")\n                    if name.startswith(\"GOOGLE_API_KEY\"):\n                        key = value.strip()\n                        break\n        if not key:\n            msg = \"no API key found (GOOGLE_API_KEY environment variable)\"\n            raise RuntimeError(msg)\n\n        self.service = discovery.build(\n            \"trends\",\n            version,\n            discoveryServiceUrl=f\"https://trends.googleapis.com/$discovery/rest?version={version}\",\n            developerKey=key,\n        )\n\n    def process_batches(\n        self,\n        start: Union[str, None] = None,\n        end: Union[str, None] = None,\n        resolution=\"week\",\n        override_terms: Union[List[str], None] = None,\n        override_location: Union[List[str], None] = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Processes collection batches from scope.\n\n        Args:\n            start (str | None): First date to collect from; `YYYY-MM-DD`.\n            end (str | None): Last date to collect from; `YYYY-MM-DD`.\n            resolution (str): Collection resolution; `day`, `week`, `month`, or `year`.\n            override_terms (str): List of terms to collect instead of those in scope.\n                Useful for changing collection order or filling out select terms.\n            override_location (str): List of locations to collect from instead of those in scope.\n\n        Examples:\n            ```python\n            # collect across all scope-defined terms and locations in 2024\n            data = collector.process_batches(\"2024-01-01\", \"2024-12-31\")\n            ```\n\n        Returns:\n            A `pandas.DataFrame` of the combined results.\n        \"\"\"\n\n        params: Dict[str, Union[List[str], str]] = {\"timelineResolution\": resolution}\n        if start:\n            params[\"time_startDate\"] = start\n        if end:\n            params[\"time_endDate\"] = end\n\n        terms = override_terms if override_terms else read_scope(self.scope_dir, \"terms\")\n        locations = override_location if override_location else read_scope(self.scope_dir, \"locations\")\n        locations = {loc if len(loc) &lt; 9 else loc.split(\"-\")[2] for loc in locations}\n\n        for term_set in range(0, len(terms), self.max_terms):\n            for location in locations:\n                batch_params = {\n                    \"terms\": terms[term_set : (term_set + self.max_terms)],\n                    **params,\n                }\n                batch_params[_location_type(location)] = location\n                batch = self.collect(location, batch_params)\n                self.batches.append(batch)\n                sleep(self._regular_wait_time)\n\n        data = concat(self.batches)\n        return data\n\n    def collect(\n        self,\n        location: str,\n        params: Dict[str, Union[List[str], str]],\n    ) -&gt; DataFrame:\n        \"\"\"\n        Collect a single batch.\n\n        Args:\n            location (str): Country (e.g., `US`), region (state; e.g., `US-AL`),\n                or DMA (metro area; e.g., `US-AL-630` or `630`) code.\n            params (dict[str, list[str] | str]): A dictionary with the following entries:\n\n                * `terms` (list[str]): List of terms to collect.\n                * `timelineResolution` (str): Collection resolution; `day`, `week`, `month`, or `year`.\n                * `time_startDate` (str): First date to collect from; `YYYY-MM-DD`.\n                * `time_endDate` (str): First date to collect from; `YYYY-MM-DD`.\n\n        Examples:\n            ```python\n            # collect a small, custom sample\n            data = collector.collect(\n                \"US-NY\",\n                {\n                    \"terms\": [\"cough\", \"/m/01b_21\"],\n                    \"timelineResolution\": \"month\",\n                    \"time_startDate\": \"2014-01-01\",\n                    \"time_endDate\": \"2024-01-01\",\n                },\n            )\n            ```\n\n        Returns:\n            A `pandas.DataFrame` of the prepared results, with these columns:\n\n                * `value`: Number indicating search volume.\n                * `date`: Date the searches were recorded on.\n                * `location`: Location code in which searches were recorded from.\n                * `term`: The search term.\n                * `retrieved`: Date retrived from the API.\n        \"\"\"\n\n        try:\n            # pylint: disable=E1101\n            response = self.service.getTimelinesForHealth(**params).execute()\n        except errors.HttpError as e:\n            if e.reason == \"rateLimitExceeded\":\n                sleep(self._fallback_wait_time)\n                response = self.collect(location, params)\n            else:\n                raise e\n        today = (datetime.datetime.now(datetime.timezone.utc)).strftime(\"%Y-%m-%d\")\n        data = []\n        for line in response[\"lines\"]:\n            points = json_normalize(line[\"points\"])\n            points[\"date\"] = to_datetime(points[\"date\"]).dt.strftime(\"%Y-%m-%d\")\n            points[\"location\"] = location\n            points[\"term\"] = line[\"term\"]\n            points[\"retrieved\"] = today\n            data.append(points)\n        return concat(data)\n</code></pre>"},{"location":"functions/Collector/#gtrends_collection.Collector.collect","title":"<code>collect(location, params)</code>","text":"<p>Collect a single batch.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>Country (e.g., <code>US</code>), region (state; e.g., <code>US-AL</code>), or DMA (metro area; e.g., <code>US-AL-630</code> or <code>630</code>) code.</p> required <code>params</code> <code>dict[str, list[str] | str]</code> <p>A dictionary with the following entries:</p> <ul> <li><code>terms</code> (list[str]): List of terms to collect.</li> <li><code>timelineResolution</code> (str): Collection resolution; <code>day</code>, <code>week</code>, <code>month</code>, or <code>year</code>.</li> <li><code>time_startDate</code> (str): First date to collect from; <code>YYYY-MM-DD</code>.</li> <li><code>time_endDate</code> (str): First date to collect from; <code>YYYY-MM-DD</code>.</li> </ul> required <p>Examples:</p> <pre><code># collect a small, custom sample\ndata = collector.collect(\n    \"US-NY\",\n    {\n        \"terms\": [\"cough\", \"/m/01b_21\"],\n        \"timelineResolution\": \"month\",\n        \"time_startDate\": \"2014-01-01\",\n        \"time_endDate\": \"2024-01-01\",\n    },\n)\n</code></pre> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A <code>pandas.DataFrame</code> of the prepared results, with these columns:</p> <ul> <li><code>value</code>: Number indicating search volume.</li> <li><code>date</code>: Date the searches were recorded on.</li> <li><code>location</code>: Location code in which searches were recorded from.</li> <li><code>term</code>: The search term.</li> <li><code>retrieved</code>: Date retrived from the API.</li> </ul> Source code in <code>gtrends_collection/collector.py</code> <pre><code>def collect(\n    self,\n    location: str,\n    params: Dict[str, Union[List[str], str]],\n) -&gt; DataFrame:\n    \"\"\"\n    Collect a single batch.\n\n    Args:\n        location (str): Country (e.g., `US`), region (state; e.g., `US-AL`),\n            or DMA (metro area; e.g., `US-AL-630` or `630`) code.\n        params (dict[str, list[str] | str]): A dictionary with the following entries:\n\n            * `terms` (list[str]): List of terms to collect.\n            * `timelineResolution` (str): Collection resolution; `day`, `week`, `month`, or `year`.\n            * `time_startDate` (str): First date to collect from; `YYYY-MM-DD`.\n            * `time_endDate` (str): First date to collect from; `YYYY-MM-DD`.\n\n    Examples:\n        ```python\n        # collect a small, custom sample\n        data = collector.collect(\n            \"US-NY\",\n            {\n                \"terms\": [\"cough\", \"/m/01b_21\"],\n                \"timelineResolution\": \"month\",\n                \"time_startDate\": \"2014-01-01\",\n                \"time_endDate\": \"2024-01-01\",\n            },\n        )\n        ```\n\n    Returns:\n        A `pandas.DataFrame` of the prepared results, with these columns:\n\n            * `value`: Number indicating search volume.\n            * `date`: Date the searches were recorded on.\n            * `location`: Location code in which searches were recorded from.\n            * `term`: The search term.\n            * `retrieved`: Date retrived from the API.\n    \"\"\"\n\n    try:\n        # pylint: disable=E1101\n        response = self.service.getTimelinesForHealth(**params).execute()\n    except errors.HttpError as e:\n        if e.reason == \"rateLimitExceeded\":\n            sleep(self._fallback_wait_time)\n            response = self.collect(location, params)\n        else:\n            raise e\n    today = (datetime.datetime.now(datetime.timezone.utc)).strftime(\"%Y-%m-%d\")\n    data = []\n    for line in response[\"lines\"]:\n        points = json_normalize(line[\"points\"])\n        points[\"date\"] = to_datetime(points[\"date\"]).dt.strftime(\"%Y-%m-%d\")\n        points[\"location\"] = location\n        points[\"term\"] = line[\"term\"]\n        points[\"retrieved\"] = today\n        data.append(points)\n    return concat(data)\n</code></pre>"},{"location":"functions/Collector/#gtrends_collection.Collector.process_batches","title":"<code>process_batches(start=None, end=None, resolution='week', override_terms=None, override_location=None)</code>","text":"<p>Processes collection batches from scope.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>str | None</code> <p>First date to collect from; <code>YYYY-MM-DD</code>.</p> <code>None</code> <code>end</code> <code>str | None</code> <p>Last date to collect from; <code>YYYY-MM-DD</code>.</p> <code>None</code> <code>resolution</code> <code>str</code> <p>Collection resolution; <code>day</code>, <code>week</code>, <code>month</code>, or <code>year</code>.</p> <code>'week'</code> <code>override_terms</code> <code>str</code> <p>List of terms to collect instead of those in scope. Useful for changing collection order or filling out select terms.</p> <code>None</code> <code>override_location</code> <code>str</code> <p>List of locations to collect from instead of those in scope.</p> <code>None</code> <p>Examples:</p> <pre><code># collect across all scope-defined terms and locations in 2024\ndata = collector.process_batches(\"2024-01-01\", \"2024-12-31\")\n</code></pre> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A <code>pandas.DataFrame</code> of the combined results.</p> Source code in <code>gtrends_collection/collector.py</code> <pre><code>def process_batches(\n    self,\n    start: Union[str, None] = None,\n    end: Union[str, None] = None,\n    resolution=\"week\",\n    override_terms: Union[List[str], None] = None,\n    override_location: Union[List[str], None] = None,\n) -&gt; DataFrame:\n    \"\"\"\n    Processes collection batches from scope.\n\n    Args:\n        start (str | None): First date to collect from; `YYYY-MM-DD`.\n        end (str | None): Last date to collect from; `YYYY-MM-DD`.\n        resolution (str): Collection resolution; `day`, `week`, `month`, or `year`.\n        override_terms (str): List of terms to collect instead of those in scope.\n            Useful for changing collection order or filling out select terms.\n        override_location (str): List of locations to collect from instead of those in scope.\n\n    Examples:\n        ```python\n        # collect across all scope-defined terms and locations in 2024\n        data = collector.process_batches(\"2024-01-01\", \"2024-12-31\")\n        ```\n\n    Returns:\n        A `pandas.DataFrame` of the combined results.\n    \"\"\"\n\n    params: Dict[str, Union[List[str], str]] = {\"timelineResolution\": resolution}\n    if start:\n        params[\"time_startDate\"] = start\n    if end:\n        params[\"time_endDate\"] = end\n\n    terms = override_terms if override_terms else read_scope(self.scope_dir, \"terms\")\n    locations = override_location if override_location else read_scope(self.scope_dir, \"locations\")\n    locations = {loc if len(loc) &lt; 9 else loc.split(\"-\")[2] for loc in locations}\n\n    for term_set in range(0, len(terms), self.max_terms):\n        for location in locations:\n            batch_params = {\n                \"terms\": terms[term_set : (term_set + self.max_terms)],\n                **params,\n            }\n            batch_params[_location_type(location)] = location\n            batch = self.collect(location, batch_params)\n            self.batches.append(batch)\n            sleep(self._regular_wait_time)\n\n    data = concat(self.batches)\n    return data\n</code></pre>"},{"location":"functions/dataset/","title":"dataset","text":"<p>Write and manage trends dataset.</p>"},{"location":"functions/dataset/#gtrends_collection.dataset.defragment_dataset","title":"<code>defragment_dataset(data_dir='data')</code>","text":"<p>Defragments the dataset partitions.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>directory of the Parquet dataset.</p> <code>'data'</code> Source code in <code>gtrends_collection/dataset.py</code> <pre><code>def defragment_dataset(data_dir=\"data\"):\n    \"\"\"\n    Defragments the dataset partitions.\n\n    Args:\n      data_dir (str): directory of the Parquet dataset.\n    \"\"\"\n    for part_name in listdir(data_dir):\n        part_dir = f\"{data_dir}/{part_name}/\"\n        part = pyarrow.dataset.dataset(\n            part_dir, gtrends_schema, format=\"parquet\", exclude_invalid_files=True\n        ).to_table()\n        pyarrow.parquet.write_table(part, f\"{part_dir}part-0.parquet\", compression=\"gzip\")\n        for fragment in glob(f\"{part_dir}fragment*.parquet\"):\n            unlink(fragment)\n</code></pre>"},{"location":"functions/dataset/#gtrends_collection.dataset.write_to_dataset","title":"<code>write_to_dataset(data, data_dir='data', defragment=True)</code>","text":"<p>Write term fragments to a Parquet dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Collection results.</p> required <code>data_dir</code> <code>str</code> <p>Directory of the Parquet dataset.</p> <code>'data'</code> <code>defragment</code> <code>bool</code> <p>If <code>True</code>, defragments the dataset after writing new fragments.</p> <code>True</code> Source code in <code>gtrends_collection/dataset.py</code> <pre><code>def write_to_dataset(data: DataFrame, data_dir=\"data\", defragment=True):\n    \"\"\"\n    Write term fragments to a Parquet dataset.\n\n    Args:\n      data (DataFrame): Collection results.\n      data_dir (str): Directory of the Parquet dataset.\n      defragment (bool): If `True`, defragments the dataset after writing new fragments.\n    \"\"\"\n    for term, group in data.groupby(\"term\"):\n        encoded_term = quote_plus(term)\n        part_dir = f\"{data_dir}/term={encoded_term}/\"\n        makedirs(part_dir, exist_ok=True)\n        pyarrow.parquet.write_table(\n            pyarrow.Table.from_pandas(group, schema=gtrends_schema),\n            f\"{part_dir}fragment-{ceil(time())!s}-0.parquet\",\n            compression=\"gzip\",\n        )\n    if defragment:\n        defragment_dataset(data_dir)\n</code></pre>"},{"location":"functions/utils/","title":"utils","text":"<p>Utility function to interact with framework resources and trends data.</p>"},{"location":"functions/utils/#gtrends_collection.utils.full_metro_area_codes","title":"<code>full_metro_area_codes(scope_dir, locations)</code>","text":"<p>Adds country and state codes to metro area codes (e.g., <code>630</code> becomes <code>US-AL-630</code>), based on <code>scope_dir/locations.txt</code>.</p> <p>Parameters:</p> Name Type Description Default <code>locations</code> <code>List[str]</code> <p>Locations to potentially prepend full location codes to.</p> required <p>Examples:</p> <pre><code>full_metro_area_codes(\"./scope\", [\"630\", \"743\"])\n</code></pre> <p>Returns:</p> Type Description <code>List[str]</code> <p>A version of <code>locations</code> with any matching locations expanded.</p> Source code in <code>gtrends_collection/utils.py</code> <pre><code>def full_metro_area_codes(scope_dir: str, locations: List[str]) -&gt; List[str]:\n    \"\"\"\n    Adds country and state codes to metro area codes (e.g., `630` becomes `US-AL-630`),\n    based on `scope_dir/locations.txt`.\n\n    Args:\n        locations (List[str]): Locations to potentially prepend full location codes to.\n\n    Examples:\n        ```python\n        full_metro_area_codes(\"./scope\", [\"630\", \"743\"])\n        ```\n\n    Returns:\n        A version of `locations` with any matching locations expanded.\n    \"\"\"\n    location_map: Dict[str, str] = {}\n    for location in read_scope(scope_dir, \"locations\"):\n        if len(location) == 9:\n            location_parts = location.split(\"-\")\n            location_map[location_parts[2]] = location\n    return [location_map.get(loc, loc) for loc in locations]\n</code></pre>"},{"location":"functions/utils/#gtrends_collection.utils.full_term_names","title":"<code>full_term_names(scope_dir, terms, include_id=True)</code>","text":"<p>Converts topic and category IDs to their full names, based on <code>scope_dir/term_map.csv</code>.</p> <p>Parameters:</p> Name Type Description Default <code>terms</code> <code>List[str]</code> <p>Terms to be converted.</p> required <p>Examples:</p> <pre><code>full_term_names(\"./scope\", [\"/m/0cycc\", \"/g/11hy9m64ws\"])\n</code></pre> <p>Returns:</p> Type Description <code>List[str]</code> <p>A version of <code>terns</code> with any matching terms converted.</p> Source code in <code>gtrends_collection/utils.py</code> <pre><code>def full_term_names(scope_dir: str, terms: List[str], include_id=True) -&gt; List[str]:\n    \"\"\"\n    Converts topic and category IDs to their full names, based on `scope_dir/term_map.csv`.\n\n    Args:\n        terms (List[str]): Terms to be converted.\n\n    Examples:\n        ```python\n        full_term_names(\"./scope\", [\"/m/0cycc\", \"/g/11hy9m64ws\"])\n        ```\n\n    Returns:\n        A version of `terns` with any matching terms converted.\n    \"\"\"\n    map_file = f\"{scope_dir}/term_map.csv\"\n    if not isfile(map_file):\n        return terms\n    term_map = pandas.read_csv(map_file, index_col=\"id\")[\"name\"].to_dict()\n    return [term_map.get(term, term) + (f\" ({term})\" if include_id and term.startswith(\"/\") else \"\") for term in terms]\n</code></pre>"},{"location":"functions/utils/#gtrends_collection.utils.read_scope","title":"<code>read_scope(scope_dir, which)</code>","text":"<p>Reads in a scope file.</p> <p>Parameters:</p> Name Type Description Default <code>scope_dir</code> <code>str</code> <p>Directory containing scope files.</p> required <code>which</code> <code>str</code> <p>Name of the file to be read in (e.g., <code>locations</code>).</p> required <p>Examples:</p> <pre><code>terms = read_scope(\"./scope\", \"terms\")\n</code></pre> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of terms or locations.</p> Source code in <code>gtrends_collection/utils.py</code> <pre><code>def read_scope(scope_dir: str, which: str) -&gt; List[str]:\n    \"\"\"\n    Reads in a scope file.\n\n    Args:\n        scope_dir (str): Directory containing scope files.\n        which (str): Name of the file to be read in (e.g., `locations`).\n\n    Examples:\n        ```python\n        terms = read_scope(\"./scope\", \"terms\")\n        ```\n\n    Returns:\n        A list of terms or locations.\n    \"\"\"\n    with open(f\"{scope_dir}/{which}.txt\", encoding=\"utf-8\") as content:\n        locations = [code.strip() for code in content.readlines()]\n    return locations\n</code></pre>"}]}