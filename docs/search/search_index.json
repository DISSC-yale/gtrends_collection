{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Google Trends Collection Framework","text":"<p>This is in part a simple Python package to handle collection from the Google Trends beta research API, and a minimal framework to organize historical and continuous collection.</p>"},{"location":"#data","title":"Data","text":"<p>A selection of data are collected in the <code>data</code> directory (weekly data updated weekly) and in the <code>data_yearly</code> directory (yearly data updated weekly).</p> <p>The selection is defined by the files in the <code>scope</code> directory.</p>"},{"location":"#local-use","title":"Local Use","text":"<p>To work with the data locally, you can clone this repository:</p> <pre><code>git clone --depth=1 https://github.com/DISSC-yale/gtrends_collection.git\n</code></pre> <p>Then load the data in Python:</p> <pre><code>from pyarrow.dataset import dataset\n\ndata = dataset(\"gtrends_collection/data\").to_table().to_pandas()\n</code></pre> <p>or R:</p> <pre><code>library(arrow)\n\ndata &lt;- open_dataset(\"gtrends_collection/data\") |&gt; dplyr::collect()\n</code></pre>"},{"location":"#collection","title":"Collection","text":"<p>The <code>scripts/historical_collection.py</code> script is used to collect full histories based on the scope files:</p> <pre><code>python scripts/historical_collection.py\n</code></pre> <p>The <code>scripts/weekly_collection.py</code> script is used by the GitHub Actions workflow to add new data each week. The <code>scripts/yearly_collection.py</code> script is used by another workflow to add yearly data each month.</p> <p>The <code>scripts/add_terms.py</code> script can be used to add and collect new terms:</p> <pre><code>python scripts/add_terms.py \"term, another term\"\n</code></pre> <p>Any topic or category terms should also be manually added to <code>scope/term_map.csv</code>.</p>"},{"location":"#authentication","title":"Authentication","text":"<p>A developer key is required to collect from the beta API.</p> <p>This can either be set to the <code>GOOGLE_API_KEY</code> environment variable, or stored in an <code>.env</code> file:</p> <pre><code>GOOGLE_API_KEY=AlphanumericKey\n</code></pre>"},{"location":"#rebuilding","title":"Rebuilding","text":""},{"location":"#scripts","title":"Scripts","text":"<p>Some scripts are used to build parts of the repository.</p>"},{"location":"#summary","title":"Summary","text":"<p>The <code>scripts/build_summary.py</code> script creates <code>docs_source/Data.md</code>:</p> <pre><code>python scripts/build_summary.py\n</code></pre> <p>The <code>scripts/build_summary_yearly.py</code> script creates <code>docs_source/Data_Yearly.md</code>:</p>"},{"location":"#metadata","title":"Metadata","text":"<p>The <code>scripts/build_metadata.py</code> script creates <code>scope/locations.txt</code>:</p> <pre><code>python scripts/build_metadata.py\n</code></pre>"},{"location":"#hatch","title":"Hatch","text":"<p>Hatch is used to manage package testing:</p> <pre><code>hatch run test\n</code></pre> <p>And building of the documentation site:</p> <pre><code>hatch run docs:build\n</code></pre>"},{"location":"#legal-disclaimer","title":"Legal Disclaimer","text":"<p>Data are provided \"as is\", without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, and noninfringement. In no event shall the authors, contributors, or copyright holders be liable for any claim, damages, or other liability, whether in an action of contract, tort, or otherwise, arising from, out of, or in connection with the data or the use or other dealings in the data.</p>"},{"location":"Data/","title":"Data","text":"<p>Summaries of the data collected as of 10:20:21 PM UTC on 2025-12-27</p>"},{"location":"Data/#locations","title":"Locations","text":"<ul> <li>Observations</li> <li>Means</li> </ul>"},{"location":"Data/#dates","title":"Dates","text":"term min max Respiratory syncytial virus (/g/11hy9m64ws) 2003-12-28 2025-12-21 Respiratory syncytial virus vaccine (/g/11j30ybfx6) 2003-12-28 2025-12-21 Nirsevimab (/g/11q4gh4b8f) 2003-12-28 2025-12-21 Bronchiolitis obliterans (/m/0b7k33) 2003-12-28 2025-12-21 Influenza (/m/0cycc) 2003-12-28 2025-12-21 9mm 2003-12-28 2025-12-21 Naloxone 2003-12-28 2025-12-21 bronchiolitis 2003-12-28 2025-12-21 drug overdose 2003-12-28 2025-12-21 heat exhaustion 2003-12-28 2025-12-21 heat stroke 2003-12-28 2025-12-21 influenza 2003-12-28 2025-12-21 narcan 2003-12-28 2025-12-21 nirsevimab 2003-12-28 2025-12-21 overdose 2003-12-28 2025-12-21 rsv 2003-12-28 2025-12-21 shotgun 2003-12-28 2025-12-21"},{"location":"Data/#values","title":"Values","text":"term min mean std max Respiratory syncytial virus (/g/11hy9m64ws) 0.00 85.36 5833.34 4434828.10 Respiratory syncytial virus vaccine (/g/11j30ybfx6) 0.00 67.23 9110.41 5368845.95 Nirsevimab (/g/11q4gh4b8f) 0.00 3.07 114.69 12466.52 Bronchiolitis obliterans (/m/0b7k33) 0.00 51.95 4580.29 2077404.38 Influenza (/m/0cycc) 0.00 2494.05 6479.62 1763593.87 9mm 0.00 724.99 5092.97 2040894.99 Naloxone 0.00 83.86 13358.70 6706513.72 bronchiolitis 0.00 49.85 5412.52 2642999.99 drug overdose 0.00 74.45 11408.21 5575236.16 heat exhaustion 0.00 54.61 2964.10 982450.34 heat stroke 0.00 64.77 3967.39 1104079.47 influenza 0.00 190.81 3617.57 1362586.92 narcan 0.00 74.98 4715.18 1643030.73 nirsevimab 0.00 32.85 2720.71 1325261.88 overdose 0.00 343.68 5606.01 2339361.70 rsv 0.00 284.83 3030.82 1175033.41 shotgun 0.00 966.10 3032.52 1194879.84"},{"location":"Data_Yearly/","title":"Data Yearly","text":"<p>Summaries of the data collected as of 10:22:56 PM UTC on 2025-12-23</p>"},{"location":"Data_Yearly/#locations","title":"Locations","text":"<ul> <li>Observations</li> <li>Means</li> </ul>"},{"location":"Data_Yearly/#dates","title":"Dates","text":"term min max Respiratory syncytial virus (/g/11hy9m64ws) 2004-01-01 2025-01-01 Respiratory syncytial virus vaccine (/g/11j30ybfx6) 2004-01-01 2025-01-01 Nirsevimab (/g/11q4gh4b8f) 2004-01-01 2025-01-01 Bronchiolitis obliterans (/m/0b7k33) 2004-01-01 2025-01-01 Influenza (/m/0cycc) 2004-01-01 2025-01-01 9mm 2004-01-01 2025-01-01 Naloxone 2004-01-01 2025-01-01 bronchiolitis 2004-01-01 2025-01-01 drug overdose 2004-01-01 2025-01-01 heat exhaustion 2004-01-01 2025-01-01 heat stroke 2004-01-01 2025-01-01 influenza 2004-01-01 2025-01-01 narcan 2004-01-01 2025-01-01 nirsevimab 2004-01-01 2025-01-01 overdose 2004-01-01 2025-01-01 rsv 2004-01-01 2025-01-01 shotgun 2004-01-01 2025-01-01"},{"location":"Data_Yearly/#values","title":"Values","text":"term min mean std max Respiratory syncytial virus (/g/11hy9m64ws) 0.00 63.04 165.04 2017.72 Respiratory syncytial virus vaccine (/g/11j30ybfx6) 0.00 28.99 86.70 4549.99 Nirsevimab (/g/11q4gh4b8f) 0.00 0.93 25.72 1874.14 Bronchiolitis obliterans (/m/0b7k33) 0.00 29.11 202.30 21542.75 Influenza (/m/0cycc) 0.00 3063.12 2077.07 19446.33 9mm 0.00 1035.15 601.79 6282.86 Naloxone 0.00 39.70 134.56 16978.00 bronchiolitis 0.00 22.59 28.49 2137.07 drug overdose 0.00 35.32 36.24 1810.10 heat exhaustion 0.00 48.30 69.11 8445.26 heat stroke 0.00 74.45 57.60 1975.56 influenza 0.00 340.85 303.76 7278.06 narcan 0.00 67.89 145.13 18190.90 nirsevimab 0.00 0.58 19.81 2724.31 overdose 0.00 604.41 201.95 3430.69 rsv 0.00 402.48 373.20 2608.04 shotgun 0.00 1511.37 623.86 15836.33"},{"location":"Framework/","title":"Framework","text":""},{"location":"Framework/#components","title":"Components","text":"<p>The Framework aspect of this repository consists of the following components:</p> <ol> <li>The <code>GOOGLE_API_KEY</code> environment variable, which may be stored in a <code>.env</code> file.</li> <li>A pair of <code>scope</code> files to define the main collection targets.</li> <li><code>scripts</code> to define different collection tasks.</li> <li>An output directory (<code>data</code> or <code>data_yearly</code>) to write results to.</li> </ol> <p>These make up a structure around which data are collected. By default, they are assumed to be in the root directory, but each can be redirected with function arguments.</p>"},{"location":"Framework/#automatic-updates","title":"Automatic Updates","text":"<p>On top of those components, the <code>.github/workflows/weekly_collection.yaml</code> and <code>.github/workflows/yearly_collection.yaml</code> files defines GitHub actions used to perform regular updates.</p> <p>This depends on the <code>GOOGLE_API_KEY</code> being defined in actions secretes (Settings &gt; Secrets and variables &gt; Actions &gt; Secrets) and actions having write permissions (Settings &gt; Actions &gt; General &gt; Workflow permissions).</p>"},{"location":"functions/Collector/","title":"Collector","text":"<p>Collect internet search volumes from the Google Trends timeline for health endpoint.</p> <p>See the schema for more about the API. Only the <code>getTimelinesForHealth</code> endpoint is used here.</p> <p>Parameters:</p> Name Type Description Default <code>scope_dir</code> <code>str</code> <p>Directory containing the <code>terms.txt</code> and <code>locations.txt</code> files. See Specification.</p> <code>'scope'</code> <code>key_dir</code> <code>str</code> <p>Directory containing a <code>.env</code> file, to extract the <code>GOOGLE_API_KEY</code> variable from, if it is not already in the environment.</p> <code>'.'</code> <code>terms_per_batch</code> <code>int</code> <p>Maximum terms to include in each collection batch. Theoretically 30 is the API's max, but more than 1 seems to not work.</p> <code>1</code> <code>wait_time</code> <code>float</code> <p>Seconds to wait between each batch.</p> <code>0.1</code> <code>version</code> <code>str</code> <p>Version of the service API.</p> <code>'v1beta'</code> Specification <p>To process in batches, search terms and locations must be specified in separate files (<code>terms.txt</code> and <code>locations.txt</code>), stored in the <code>scope_dir</code> directory. These should contain 1 term / location code per line.</p> Collection Process <p>Initializing this class retrieves the Google API service, stores the developer key, and points to the scope directory.</p> <p>The <code>process_batches()</code> method reads in the terms and locations, and collects them in batches over the specified time frame.</p> <p>Results from each batch are stored in the <code>batches</code> property, which can be pulled from in case the <code>process_batches</code> process does not complete (such as if the daily rate limit is reached).</p> <p>The <code>collect()</code> method collects a single batch, and can be used on its own.</p> <p>Examples:</p> <pre><code>from gtrends_collection import Collector\n\n# initialize the collector\ncollector = Collector()\n</code></pre> Source code in <code>gtrends_collection/collector.py</code> <pre><code>class Collector:\n    \"\"\"\n    Collect internet search volumes from the Google Trends timeline for health endpoint.\n\n    See the [schema](https://trends.googleapis.com/$discovery/rest?version=v1beta)\n    for more about the API. Only the `getTimelinesForHealth` endpoint is used here.\n\n    Args:\n        scope_dir (str): Directory containing the `terms.txt` and `locations.txt` files.\n            See Specification.\n        key_dir (str): Directory containing a `.env` file, to extract the\n            `GOOGLE_API_KEY` variable from, if it is not already in the environment.\n        terms_per_batch (int): Maximum terms to include in each collection batch.\n            Theoretically 30 is the API's max, but more than 1 seems to not work.\n        wait_time (float): Seconds to wait between each batch.\n        version (str): Version of the service API.\n\n    Specification:\n        To process in batches, search terms and locations must be specified in separate\n        files (`terms.txt` and `locations.txt`), stored in the `scope_dir` directory.\n        These should contain 1 term / location code per line.\n\n    Collection Process:\n        Initializing this class retrieves the Google API service, stores the\n        developer key, and points to the scope directory.\n\n        The `process_batches()` method reads in the terms and locations,\n        and collects them in batches over the specified time frame.\n\n        Results from each batch are stored in the `batches` property,\n        which can be pulled from in case the `process_batches` process does not complete\n        (such as if the daily rate limit is reached).\n\n        The `collect()` method collects a single batch, and\n        can be used on its own.\n\n    Examples:\n        ```python\n        from gtrends_collection import Collector\n\n        # initialize the collector\n        collector = Collector()\n        ```\n    \"\"\"\n\n    # time to wait between requests\n    _regular_wait_time = 0.1\n    # time to wait after a `rateLimitExceeded` error\n    _fallback_wait_time = 2\n    batches: ClassVar[List[DataFrame]] = []\n\n    scope_dir = \"scope\"\n    max_terms = 1\n\n    def __init__(\n        self,\n        scope_dir: str = \"scope\",\n        key_dir: str = \".\",\n        terms_per_batch: int = 1,\n        wait_time: float = 0.1,\n        version: str = \"v1beta\",\n    ):\n        self._regular_wait_time = wait_time\n        self.scope_dir = scope_dir\n        self.max_terms = terms_per_batch\n\n        key = getenv(\"GOOGLE_API_KEY\")\n        if not key and isfile(f\"{key_dir}/.env\"):\n            with open(f\"{key_dir}/.env\", encoding=\"utf-8\") as content:\n                for pair in content.read().split(\"\\n\"):\n                    name, value = pair.split(\"=\")\n                    if name.startswith(\"GOOGLE_API_KEY\"):\n                        key = value.strip()\n                        break\n        if not key:\n            msg = \"no API key found (GOOGLE_API_KEY environment variable)\"\n            raise RuntimeError(msg)\n\n        self.service = discovery.build(\n            \"trends\",\n            version,\n            discoveryServiceUrl=f\"https://trends.googleapis.com/$discovery/rest?version={version}\",\n            developerKey=key,\n        )\n\n    def process_batches(\n        self,\n        start: Union[str, None] = None,\n        end: Union[str, None] = None,\n        resolution: str = \"week\",\n        override_terms: Union[List[str], None] = None,\n        override_location: Union[List[str], None] = None,\n    ) -&gt; DataFrame:\n        \"\"\"\n        Processes collection batches from scope.\n\n        Args:\n            start (str | None): First date to collect from; `YYYY-MM-DD`.\n            end (str | None): Last date to collect from; `YYYY-MM-DD`.\n            resolution (str): Collection resolution; `day`, `week`, `month`, or `year`.\n            override_terms (str): List of terms to collect instead of those in scope.\n                Useful for changing collection order or filling out select terms.\n            override_location (str): List of locations to collect from instead of those in scope.\n\n        Examples:\n            ```python\n            # collect across all scope-defined terms and locations in 2024\n            data = collector.process_batches(\"2024-01-01\", \"2024-12-31\")\n            ```\n\n        Returns:\n            A `pandas.DataFrame` of the combined results.\n        \"\"\"\n\n        params: Dict[str, Union[List[str], str]] = {\"timelineResolution\": resolution}\n        if start:\n            params[\"time_startDate\"] = start\n        if end:\n            params[\"time_endDate\"] = end\n\n        terms = override_terms if override_terms else read_scope(self.scope_dir, \"terms\")\n        locations = override_location if override_location else read_scope(self.scope_dir, \"locations\")\n        locations = {loc if len(loc) &lt; 9 else loc.split(\"-\")[2] for loc in locations}\n\n        for term_set in range(0, len(terms), self.max_terms):\n            for location in locations:\n                batch_params = {\n                    \"terms\": terms[term_set : (term_set + self.max_terms)],\n                    **params,\n                }\n                batch_params[_location_type(location)] = location\n                batch = self.collect(location, batch_params)\n                self.batches.append(batch)\n                sleep(self._regular_wait_time)\n\n        data = concat(self.batches)\n        return data\n\n    def collect(\n        self,\n        location: str,\n        params: Dict[str, Union[List[str], str]],\n    ) -&gt; DataFrame:\n        \"\"\"\n        Collect a single batch.\n\n        Args:\n            location (str): Country (e.g., `US`), region (state; e.g., `US-AL`),\n                or DMA (metro area; e.g., `US-AL-630` or `630`) code.\n            params (dict[str, list[str] | str]): A dictionary with the following entries:\n\n                * `terms` (list[str]): List of terms to collect.\n                * `timelineResolution` (str): Collection resolution; `day`, `week`, `month`, or `year`.\n                * `time_startDate` (str): First date to collect from; `YYYY-MM-DD`.\n                * `time_endDate` (str): First date to collect from; `YYYY-MM-DD`.\n\n        Examples:\n            ```python\n            # collect a small, custom sample\n            data = collector.collect(\n                \"US-NY\",\n                {\n                    \"terms\": [\"cough\", \"/m/01b_21\"],\n                    \"timelineResolution\": \"month\",\n                    \"time_startDate\": \"2014-01-01\",\n                    \"time_endDate\": \"2024-01-01\",\n                },\n            )\n            ```\n\n        Returns:\n            A `pandas.DataFrame` of the prepared results, with these columns:\n\n                * `value`: Number indicating search volume.\n                * `date`: Date the searches were recorded on.\n                * `location`: Location code in which searches were recorded from.\n                * `term`: The search term.\n                * `retrieved`: Date retrived from the API.\n        \"\"\"\n\n        try:\n            # pylint: disable=E1101\n            response = self.service.getTimelinesForHealth(**params).execute()\n        except errors.HttpError as e:\n            if e.status_code == 429:\n                sleep(self._fallback_wait_time)\n                return self.collect(location, params)\n            raise e\n        today = (datetime.datetime.now(datetime.timezone.utc)).strftime(\"%Y-%m-%d\")\n        data = []\n        for line in response[\"lines\"]:\n            points = json_normalize(line[\"points\"])\n            points[\"date\"] = to_datetime(points[\"date\"], format=\"mixed\").dt.strftime(\"%Y-%m-%d\")\n            points[\"location\"] = location\n            points[\"term\"] = line[\"term\"]\n            points[\"retrieved\"] = today\n            data.append(points)\n        return concat(data)\n</code></pre>"},{"location":"functions/Collector/#gtrends_collection.Collector.collect","title":"<code>collect(location, params)</code>","text":"<p>Collect a single batch.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>Country (e.g., <code>US</code>), region (state; e.g., <code>US-AL</code>), or DMA (metro area; e.g., <code>US-AL-630</code> or <code>630</code>) code.</p> required <code>params</code> <code>dict[str, list[str] | str]</code> <p>A dictionary with the following entries:</p> <ul> <li><code>terms</code> (list[str]): List of terms to collect.</li> <li><code>timelineResolution</code> (str): Collection resolution; <code>day</code>, <code>week</code>, <code>month</code>, or <code>year</code>.</li> <li><code>time_startDate</code> (str): First date to collect from; <code>YYYY-MM-DD</code>.</li> <li><code>time_endDate</code> (str): First date to collect from; <code>YYYY-MM-DD</code>.</li> </ul> required <p>Examples:</p> <pre><code># collect a small, custom sample\ndata = collector.collect(\n    \"US-NY\",\n    {\n        \"terms\": [\"cough\", \"/m/01b_21\"],\n        \"timelineResolution\": \"month\",\n        \"time_startDate\": \"2014-01-01\",\n        \"time_endDate\": \"2024-01-01\",\n    },\n)\n</code></pre> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A <code>pandas.DataFrame</code> of the prepared results, with these columns:</p> <ul> <li><code>value</code>: Number indicating search volume.</li> <li><code>date</code>: Date the searches were recorded on.</li> <li><code>location</code>: Location code in which searches were recorded from.</li> <li><code>term</code>: The search term.</li> <li><code>retrieved</code>: Date retrived from the API.</li> </ul> Source code in <code>gtrends_collection/collector.py</code> <pre><code>def collect(\n    self,\n    location: str,\n    params: Dict[str, Union[List[str], str]],\n) -&gt; DataFrame:\n    \"\"\"\n    Collect a single batch.\n\n    Args:\n        location (str): Country (e.g., `US`), region (state; e.g., `US-AL`),\n            or DMA (metro area; e.g., `US-AL-630` or `630`) code.\n        params (dict[str, list[str] | str]): A dictionary with the following entries:\n\n            * `terms` (list[str]): List of terms to collect.\n            * `timelineResolution` (str): Collection resolution; `day`, `week`, `month`, or `year`.\n            * `time_startDate` (str): First date to collect from; `YYYY-MM-DD`.\n            * `time_endDate` (str): First date to collect from; `YYYY-MM-DD`.\n\n    Examples:\n        ```python\n        # collect a small, custom sample\n        data = collector.collect(\n            \"US-NY\",\n            {\n                \"terms\": [\"cough\", \"/m/01b_21\"],\n                \"timelineResolution\": \"month\",\n                \"time_startDate\": \"2014-01-01\",\n                \"time_endDate\": \"2024-01-01\",\n            },\n        )\n        ```\n\n    Returns:\n        A `pandas.DataFrame` of the prepared results, with these columns:\n\n            * `value`: Number indicating search volume.\n            * `date`: Date the searches were recorded on.\n            * `location`: Location code in which searches were recorded from.\n            * `term`: The search term.\n            * `retrieved`: Date retrived from the API.\n    \"\"\"\n\n    try:\n        # pylint: disable=E1101\n        response = self.service.getTimelinesForHealth(**params).execute()\n    except errors.HttpError as e:\n        if e.status_code == 429:\n            sleep(self._fallback_wait_time)\n            return self.collect(location, params)\n        raise e\n    today = (datetime.datetime.now(datetime.timezone.utc)).strftime(\"%Y-%m-%d\")\n    data = []\n    for line in response[\"lines\"]:\n        points = json_normalize(line[\"points\"])\n        points[\"date\"] = to_datetime(points[\"date\"], format=\"mixed\").dt.strftime(\"%Y-%m-%d\")\n        points[\"location\"] = location\n        points[\"term\"] = line[\"term\"]\n        points[\"retrieved\"] = today\n        data.append(points)\n    return concat(data)\n</code></pre>"},{"location":"functions/Collector/#gtrends_collection.Collector.process_batches","title":"<code>process_batches(start=None, end=None, resolution='week', override_terms=None, override_location=None)</code>","text":"<p>Processes collection batches from scope.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>str | None</code> <p>First date to collect from; <code>YYYY-MM-DD</code>.</p> <code>None</code> <code>end</code> <code>str | None</code> <p>Last date to collect from; <code>YYYY-MM-DD</code>.</p> <code>None</code> <code>resolution</code> <code>str</code> <p>Collection resolution; <code>day</code>, <code>week</code>, <code>month</code>, or <code>year</code>.</p> <code>'week'</code> <code>override_terms</code> <code>str</code> <p>List of terms to collect instead of those in scope. Useful for changing collection order or filling out select terms.</p> <code>None</code> <code>override_location</code> <code>str</code> <p>List of locations to collect from instead of those in scope.</p> <code>None</code> <p>Examples:</p> <pre><code># collect across all scope-defined terms and locations in 2024\ndata = collector.process_batches(\"2024-01-01\", \"2024-12-31\")\n</code></pre> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A <code>pandas.DataFrame</code> of the combined results.</p> Source code in <code>gtrends_collection/collector.py</code> <pre><code>def process_batches(\n    self,\n    start: Union[str, None] = None,\n    end: Union[str, None] = None,\n    resolution: str = \"week\",\n    override_terms: Union[List[str], None] = None,\n    override_location: Union[List[str], None] = None,\n) -&gt; DataFrame:\n    \"\"\"\n    Processes collection batches from scope.\n\n    Args:\n        start (str | None): First date to collect from; `YYYY-MM-DD`.\n        end (str | None): Last date to collect from; `YYYY-MM-DD`.\n        resolution (str): Collection resolution; `day`, `week`, `month`, or `year`.\n        override_terms (str): List of terms to collect instead of those in scope.\n            Useful for changing collection order or filling out select terms.\n        override_location (str): List of locations to collect from instead of those in scope.\n\n    Examples:\n        ```python\n        # collect across all scope-defined terms and locations in 2024\n        data = collector.process_batches(\"2024-01-01\", \"2024-12-31\")\n        ```\n\n    Returns:\n        A `pandas.DataFrame` of the combined results.\n    \"\"\"\n\n    params: Dict[str, Union[List[str], str]] = {\"timelineResolution\": resolution}\n    if start:\n        params[\"time_startDate\"] = start\n    if end:\n        params[\"time_endDate\"] = end\n\n    terms = override_terms if override_terms else read_scope(self.scope_dir, \"terms\")\n    locations = override_location if override_location else read_scope(self.scope_dir, \"locations\")\n    locations = {loc if len(loc) &lt; 9 else loc.split(\"-\")[2] for loc in locations}\n\n    for term_set in range(0, len(terms), self.max_terms):\n        for location in locations:\n            batch_params = {\n                \"terms\": terms[term_set : (term_set + self.max_terms)],\n                **params,\n            }\n            batch_params[_location_type(location)] = location\n            batch = self.collect(location, batch_params)\n            self.batches.append(batch)\n            sleep(self._regular_wait_time)\n\n    data = concat(self.batches)\n    return data\n</code></pre>"},{"location":"functions/dataset/","title":"dataset","text":"<p>Write and manage trends dataset.</p>"},{"location":"functions/dataset/#gtrends_collection.dataset.defragment_dataset","title":"<code>defragment_dataset(data_dir='data')</code>","text":"<p>Defragments the dataset partitions.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>directory of the Parquet dataset.</p> <code>'data'</code> Source code in <code>gtrends_collection/dataset.py</code> <pre><code>def defragment_dataset(data_dir: str = \"data\"):\n    \"\"\"\n    Defragments the dataset partitions.\n\n    Args:\n      data_dir (str): directory of the Parquet dataset.\n    \"\"\"\n    for part_name in listdir(data_dir):\n        part_dir = f\"{data_dir}/{part_name}/\"\n        part = pyarrow.dataset.dataset(\n            part_dir, gtrends_schema, format=\"parquet\", exclude_invalid_files=True\n        ).to_table()\n        pyarrow.parquet.write_table(part, f\"{part_dir}part-0.parquet\", compression=\"gzip\")\n        for fragment in glob(f\"{part_dir}fragment*.parquet\"):\n            unlink(fragment)\n</code></pre>"},{"location":"functions/dataset/#gtrends_collection.dataset.write_to_dataset","title":"<code>write_to_dataset(data, data_dir='data', defragment=True)</code>","text":"<p>Write term fragments to a Parquet dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Collection results.</p> required <code>data_dir</code> <code>str</code> <p>Directory of the Parquet dataset.</p> <code>'data'</code> <code>defragment</code> <code>bool</code> <p>If <code>True</code>, defragments the dataset after writing new fragments.</p> <code>True</code> Source code in <code>gtrends_collection/dataset.py</code> <pre><code>def write_to_dataset(data: DataFrame, data_dir: str = \"data\", defragment: bool = True):\n    \"\"\"\n    Write term fragments to a Parquet dataset.\n\n    Args:\n      data (DataFrame): Collection results.\n      data_dir (str): Directory of the Parquet dataset.\n      defragment (bool): If `True`, defragments the dataset after writing new fragments.\n    \"\"\"\n    for term, group in data.groupby(\"term\"):\n        encoded_term = quote_plus(term)\n        part_dir = f\"{data_dir}/term={encoded_term}/\"\n        makedirs(part_dir, exist_ok=True)\n        pyarrow.parquet.write_table(\n            pyarrow.Table.from_pandas(group, schema=gtrends_schema),\n            f\"{part_dir}fragment-{ceil(time())!s}-0.parquet\",\n            compression=\"gzip\",\n        )\n    if defragment:\n        defragment_dataset(data_dir)\n</code></pre>"},{"location":"functions/utils/","title":"utils","text":"<p>Utility function to interact with framework resources and trends data.</p>"},{"location":"functions/utils/#gtrends_collection.utils.full_metro_area_codes","title":"<code>full_metro_area_codes(scope_dir, locations)</code>","text":"<p>Adds country and state codes to metro area codes (e.g., <code>630</code> becomes <code>US-AL-630</code>), based on <code>scope_dir/locations.txt</code>.</p> <p>Parameters:</p> Name Type Description Default <code>locations</code> <code>List[str]</code> <p>Locations to potentially prepend full location codes to.</p> required <p>Examples:</p> <pre><code>full_metro_area_codes(\"./scope\", [\"630\", \"743\"])\n</code></pre> <p>Returns:</p> Type Description <code>List[str]</code> <p>A version of <code>locations</code> with any matching locations expanded.</p> Source code in <code>gtrends_collection/utils.py</code> <pre><code>def full_metro_area_codes(scope_dir: str, locations: List[str]) -&gt; List[str]:\n    \"\"\"\n    Adds country and state codes to metro area codes (e.g., `630` becomes `US-AL-630`),\n    based on `scope_dir/locations.txt`.\n\n    Args:\n        locations (List[str]): Locations to potentially prepend full location codes to.\n\n    Examples:\n        ```python\n        full_metro_area_codes(\"./scope\", [\"630\", \"743\"])\n        ```\n\n    Returns:\n        A version of `locations` with any matching locations expanded.\n    \"\"\"\n    location_map: Dict[str, str] = {}\n    for location in read_scope(scope_dir, \"locations\"):\n        if len(location) == 9:\n            location_parts = location.split(\"-\")\n            location_map[location_parts[2]] = location\n    return [location_map.get(loc, loc) for loc in locations]\n</code></pre>"},{"location":"functions/utils/#gtrends_collection.utils.full_term_names","title":"<code>full_term_names(scope_dir, terms, include_id=True)</code>","text":"<p>Converts topic and category IDs to their full names, based on <code>scope_dir/term_map.csv</code>.</p> <p>Parameters:</p> Name Type Description Default <code>terms</code> <code>List[str]</code> <p>Terms to be converted.</p> required <p>Examples:</p> <pre><code>full_term_names(\"./scope\", [\"/m/0cycc\", \"/g/11hy9m64ws\"])\n</code></pre> <p>Returns:</p> Type Description <code>List[str]</code> <p>A version of <code>terns</code> with any matching terms converted.</p> Source code in <code>gtrends_collection/utils.py</code> <pre><code>def full_term_names(scope_dir: str, terms: List[str], include_id: bool = True) -&gt; List[str]:\n    \"\"\"\n    Converts topic and category IDs to their full names, based on `scope_dir/term_map.csv`.\n\n    Args:\n        terms (List[str]): Terms to be converted.\n\n    Examples:\n        ```python\n        full_term_names(\"./scope\", [\"/m/0cycc\", \"/g/11hy9m64ws\"])\n        ```\n\n    Returns:\n        A version of `terns` with any matching terms converted.\n    \"\"\"\n    map_file = f\"{scope_dir}/term_map.csv\"\n    if not isfile(map_file):\n        return terms\n    term_map = pandas.read_csv(map_file, index_col=\"id\")[\"name\"].to_dict()\n    return [term_map.get(term, term) + (f\" ({term})\" if include_id and term.startswith(\"/\") else \"\") for term in terms]\n</code></pre>"},{"location":"functions/utils/#gtrends_collection.utils.read_scope","title":"<code>read_scope(scope_dir, which)</code>","text":"<p>Reads in a scope file.</p> <p>Parameters:</p> Name Type Description Default <code>scope_dir</code> <code>str</code> <p>Directory containing scope files.</p> required <code>which</code> <code>str</code> <p>Name of the file to be read in (e.g., <code>locations</code>).</p> required <p>Examples:</p> <pre><code>terms = read_scope(\"./scope\", \"terms\")\n</code></pre> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of terms or locations.</p> Source code in <code>gtrends_collection/utils.py</code> <pre><code>def read_scope(scope_dir: str, which: str) -&gt; List[str]:\n    \"\"\"\n    Reads in a scope file.\n\n    Args:\n        scope_dir (str): Directory containing scope files.\n        which (str): Name of the file to be read in (e.g., `locations`).\n\n    Examples:\n        ```python\n        terms = read_scope(\"./scope\", \"terms\")\n        ```\n\n    Returns:\n        A list of terms or locations.\n    \"\"\"\n    with open(f\"{scope_dir}/{which}.txt\", encoding=\"utf-8\") as content:\n        locations = [code.strip() for code in content.readlines()]\n    return locations\n</code></pre>"}]}